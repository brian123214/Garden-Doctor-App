{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "actual_efficient.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMUWxQ+9xoZdcYKvL94qHAv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brian654321/Garden-Doctor-App/blob/main/actual_efficient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH8edWOBcX3P"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.backend import is_keras_tensor\n",
        "from keras import backend as K\n",
        "from keras.layers import Activation, Dense, Input, BatchNormalization, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, multiply, Permute, Concatenate, Add, Lambda, Dropout, Flatten\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import get_source_inputs\n",
        "from keras.activations import sigmoid\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from IPython.display import Image, display\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "import matplotlib.cm as cm\n",
        "import PIL.Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import cv2\n",
        "is_keras_tensor = K.is_keras_tensor\n",
        "\n",
        "\n",
        "data_directory = r\"/Users/User/Desktop/new_plant_diseaes\"\n",
        "categories = [\n",
        "    'Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Cherry___healthy', \n",
        "    'Cherry___Powdery_mildew', 'Corn___Cercospora_leaf_spot Gray_leaf_spot', 'Corn___Common_rust', \n",
        "    'Corn___healthy', 'Corn___Northern_Leaf_Blight', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Grape___healthy', \n",
        "    'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Orange___Haunglongbing_(Citrus_greening)', 'Peach___Bacterial_spot', 'Peach___healthy', \n",
        "    'Pepper,_bell___Bacterial_spot', 'Pepper,_bell___healthy', 'Potato___Early_blight', 'Potato___healthy', 'Potato___Late_blight', \n",
        "    'Squash___Powdery_mildew', 'Strawberry___healthy', 'Strawberry___Leaf_scorch', \n",
        "    'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___healthy', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', \n",
        "    'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Tomato_mosaic_virus', \n",
        "    'Tomato___Tomato_Yellow_Leaf_Curl_Virus'\n",
        "    ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw4wxu9xclR4",
        "outputId": "2115a794-ea4c-4dd2-9b88-814b999a46ba"
      },
      "source": [
        "img_size = 224\n",
        "\n",
        "def create_dataset():\n",
        "    data_set = []\n",
        "    X = []\n",
        "    y = []\n",
        "    for category in categories:\n",
        "        print(category)\n",
        "        path = os.path.join(data_directory, category)\n",
        "        class_num = categories.index(category)\n",
        "        list_directory = os.listdir(path)\n",
        "        # Only 1000 images per class\n",
        "        for i in range(1000):\n",
        "            image = list_directory[i]\n",
        "            img_array = cv2.imread(os.path.join(path, image))\n",
        "            resized_array = cv2.resize(img_array, (img_size, img_size))\n",
        "            # plt.imshow(resized_array)\n",
        "            # plt.show()\n",
        "            data_set.append([resized_array, class_num])\n",
        "            X.append(resized_array)\n",
        "            y.append(class_num)\n",
        "    X = np.array(X).reshape(-1, img_size, img_size, 3)\n",
        "    y = np.array(y)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
        "    X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size = 0.5, random_state = 0)\n",
        "    return X_train, y_train, X_test, y_test, X_val, y_val\n",
        "  \n",
        "X_train, y_train, X_test, y_test, X_val, y_val = create_dataset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple___Apple_scab\n",
            "Apple___Black_rot\n",
            "Apple___Cedar_apple_rust\n",
            "Apple___healthy\n",
            "Cherry___healthy\n",
            "Cherry___Powdery_mildew\n",
            "Corn___Cercospora_leaf_spot Gray_leaf_spot\n",
            "Corn___Common_rust\n",
            "Corn___healthy\n",
            "Corn___Northern_Leaf_Blight\n",
            "Grape___Black_rot\n",
            "Grape___Esca_(Black_Measles)\n",
            "Grape___healthy\n",
            "Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\n",
            "Orange___Haunglongbing_(Citrus_greening)\n",
            "Peach___Bacterial_spot\n",
            "Peach___healthy\n",
            "Pepper,_bell___Bacterial_spot\n",
            "Pepper,_bell___healthy\n",
            "Potato___Early_blight\n",
            "Potato___healthy\n",
            "Potato___Late_blight\n",
            "Squash___Powdery_mildew\n",
            "Strawberry___healthy\n",
            "Strawberry___Leaf_scorch\n",
            "Tomato___Bacterial_spot\n",
            "Tomato___Early_blight\n",
            "Tomato___healthy\n",
            "Tomato___Late_blight\n",
            "Tomato___Leaf_Mold\n",
            "Tomato___Septoria_leaf_spot\n",
            "Tomato___Spider_mites Two-spotted_spider_mite\n",
            "Tomato___Tomato_mosaic_virus\n",
            "Tomato___Tomato_Yellow_Leaf_Curl_Virus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCWiUOUcclUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27d7794-c88a-4577-9a38-85f705bed903"
      },
      "source": [
        "\n",
        "\n",
        "IMG_SHAPE = (img_size, img_size, 3)\n",
        "conv_base = tf.keras.applications.EfficientNetB3(weights=\"imagenet\", include_top=False, input_shape=IMG_SHAPE)\n",
        "model = tf.keras.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(tf.keras.layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "#avoid overfitting\n",
        "model.add(tf.keras.layers.Dropout(rate=0.2, name=\"dropout_out\"))\n",
        "# Set NUMBER_OF_CLASSES to the number of your final predictions.\n",
        "model.add(tf.keras.layers.Dense(len(categories), activation=\"softmax\", name=\"fc_out\"))\n",
        "conv_base.trainable = False\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.RMSprop(lr=2e-5),\n",
        "    metrics=[\"acc\"],\n",
        ")\n",
        "\n",
        "# base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "#                                               include_top=False,\n",
        "#                                               weights='imagenet')\n",
        "# base_model.trainable = False\n",
        "# model = tf.keras.Sequential([\n",
        "#                           base_model,\n",
        "#                           tf.keras.layers.GlobalAveragePooling2D(),\n",
        "#                           tf.keras.layers.Dense(34, activation='softmax')])\n",
        " \n",
        "# model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),\n",
        "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "#               metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_val, y_val))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "744/744 [==============================] - 208s 234ms/step - loss: 5.4370 - acc: 0.0567 - val_loss: 2.7926 - val_acc: 0.2459\n",
            "Epoch 2/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 3.5868 - acc: 0.1903 - val_loss: 1.8766 - val_acc: 0.4608\n",
            "Epoch 3/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 2.6896 - acc: 0.3162 - val_loss: 1.4187 - val_acc: 0.5878\n",
            "Epoch 4/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 2.2056 - acc: 0.4155 - val_loss: 1.1483 - val_acc: 0.6696\n",
            "Epoch 5/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 1.8586 - acc: 0.4853 - val_loss: 0.9772 - val_acc: 0.7114\n",
            "Epoch 6/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 1.6397 - acc: 0.5373 - val_loss: 0.8525 - val_acc: 0.7471\n",
            "Epoch 7/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 1.4601 - acc: 0.5833 - val_loss: 0.7701 - val_acc: 0.7688\n",
            "Epoch 8/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 1.3096 - acc: 0.6196 - val_loss: 0.7040 - val_acc: 0.7878\n",
            "Epoch 9/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 1.2253 - acc: 0.6426 - val_loss: 0.6521 - val_acc: 0.7994\n",
            "Epoch 10/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 1.1174 - acc: 0.6713 - val_loss: 0.6073 - val_acc: 0.8102\n",
            "Epoch 11/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 1.0649 - acc: 0.6846 - val_loss: 0.5656 - val_acc: 0.8237\n",
            "Epoch 12/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.9848 - acc: 0.7096 - val_loss: 0.5323 - val_acc: 0.8324\n",
            "Epoch 13/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.9269 - acc: 0.7206 - val_loss: 0.5144 - val_acc: 0.8386\n",
            "Epoch 14/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.9080 - acc: 0.7286 - val_loss: 0.4843 - val_acc: 0.8471\n",
            "Epoch 15/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.8378 - acc: 0.7490 - val_loss: 0.4661 - val_acc: 0.8525\n",
            "Epoch 16/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.8194 - acc: 0.7534 - val_loss: 0.4557 - val_acc: 0.8578\n",
            "Epoch 17/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.7946 - acc: 0.7617 - val_loss: 0.4333 - val_acc: 0.8661\n",
            "Epoch 18/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.7519 - acc: 0.7745 - val_loss: 0.4211 - val_acc: 0.8698\n",
            "Epoch 19/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.7312 - acc: 0.7823 - val_loss: 0.4075 - val_acc: 0.8718\n",
            "Epoch 20/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.7063 - acc: 0.7806 - val_loss: 0.3945 - val_acc: 0.8771\n",
            "Epoch 21/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.6879 - acc: 0.7888 - val_loss: 0.3813 - val_acc: 0.8782\n",
            "Epoch 22/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.6519 - acc: 0.8014 - val_loss: 0.3724 - val_acc: 0.8812\n",
            "Epoch 23/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.6654 - acc: 0.7966 - val_loss: 0.3657 - val_acc: 0.8861\n",
            "Epoch 24/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.6301 - acc: 0.8073 - val_loss: 0.3611 - val_acc: 0.8900\n",
            "Epoch 25/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.6121 - acc: 0.8095 - val_loss: 0.3495 - val_acc: 0.8888\n",
            "Epoch 26/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.5785 - acc: 0.8201 - val_loss: 0.3428 - val_acc: 0.8925\n",
            "Epoch 27/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.5674 - acc: 0.8250 - val_loss: 0.3332 - val_acc: 0.8963\n",
            "Epoch 28/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.5736 - acc: 0.8165 - val_loss: 0.3285 - val_acc: 0.8971\n",
            "Epoch 29/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.5467 - acc: 0.8306 - val_loss: 0.3228 - val_acc: 0.8975\n",
            "Epoch 30/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.5381 - acc: 0.8302 - val_loss: 0.3201 - val_acc: 0.8976\n",
            "Epoch 31/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.5448 - acc: 0.8337 - val_loss: 0.3119 - val_acc: 0.9002\n",
            "Epoch 32/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.5134 - acc: 0.8409 - val_loss: 0.3062 - val_acc: 0.9033\n",
            "Epoch 33/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.5099 - acc: 0.8409 - val_loss: 0.3024 - val_acc: 0.9029\n",
            "Epoch 34/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4994 - acc: 0.8432 - val_loss: 0.2977 - val_acc: 0.9035\n",
            "Epoch 35/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.4898 - acc: 0.8483 - val_loss: 0.2942 - val_acc: 0.9096\n",
            "Epoch 36/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4711 - acc: 0.8524 - val_loss: 0.2919 - val_acc: 0.9067\n",
            "Epoch 37/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4715 - acc: 0.8496 - val_loss: 0.2839 - val_acc: 0.9092\n",
            "Epoch 38/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4677 - acc: 0.8526 - val_loss: 0.2826 - val_acc: 0.9071\n",
            "Epoch 39/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4569 - acc: 0.8548 - val_loss: 0.2808 - val_acc: 0.9110\n",
            "Epoch 40/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4434 - acc: 0.8569 - val_loss: 0.2710 - val_acc: 0.9145\n",
            "Epoch 41/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4545 - acc: 0.8559 - val_loss: 0.2703 - val_acc: 0.9145\n",
            "Epoch 42/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4393 - acc: 0.8606 - val_loss: 0.2668 - val_acc: 0.9151\n",
            "Epoch 43/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4324 - acc: 0.8642 - val_loss: 0.2662 - val_acc: 0.9131\n",
            "Epoch 44/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4461 - acc: 0.8569 - val_loss: 0.2589 - val_acc: 0.9167\n",
            "Epoch 45/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.4257 - acc: 0.8664 - val_loss: 0.2604 - val_acc: 0.9143\n",
            "Epoch 46/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4073 - acc: 0.8703 - val_loss: 0.2538 - val_acc: 0.9200\n",
            "Epoch 47/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4067 - acc: 0.8661 - val_loss: 0.2570 - val_acc: 0.9165\n",
            "Epoch 48/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3878 - acc: 0.8721 - val_loss: 0.2549 - val_acc: 0.9190\n",
            "Epoch 49/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.4116 - acc: 0.8699 - val_loss: 0.2504 - val_acc: 0.9192\n",
            "Epoch 50/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3997 - acc: 0.8738 - val_loss: 0.2474 - val_acc: 0.9204\n",
            "Epoch 51/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.3873 - acc: 0.8767 - val_loss: 0.2497 - val_acc: 0.9206\n",
            "Epoch 52/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.3923 - acc: 0.8761 - val_loss: 0.2410 - val_acc: 0.9233\n",
            "Epoch 53/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3734 - acc: 0.8805 - val_loss: 0.2399 - val_acc: 0.9220\n",
            "Epoch 54/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3723 - acc: 0.8822 - val_loss: 0.2361 - val_acc: 0.9233\n",
            "Epoch 55/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3680 - acc: 0.8813 - val_loss: 0.2361 - val_acc: 0.9257\n",
            "Epoch 56/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3820 - acc: 0.8808 - val_loss: 0.2376 - val_acc: 0.9257\n",
            "Epoch 57/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.3733 - acc: 0.8839 - val_loss: 0.2315 - val_acc: 0.9265\n",
            "Epoch 58/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3550 - acc: 0.8870 - val_loss: 0.2314 - val_acc: 0.9267\n",
            "Epoch 59/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3658 - acc: 0.8832 - val_loss: 0.2317 - val_acc: 0.9245\n",
            "Epoch 60/200\n",
            "744/744 [==============================] - 171s 229ms/step - loss: 0.3525 - acc: 0.8862 - val_loss: 0.2293 - val_acc: 0.9267\n",
            "Epoch 61/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.3438 - acc: 0.8889 - val_loss: 0.2284 - val_acc: 0.9263\n",
            "Epoch 62/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3469 - acc: 0.8903 - val_loss: 0.2238 - val_acc: 0.9275\n",
            "Epoch 63/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3397 - acc: 0.8919 - val_loss: 0.2242 - val_acc: 0.9271\n",
            "Epoch 64/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.3301 - acc: 0.8915 - val_loss: 0.2211 - val_acc: 0.9284\n",
            "Epoch 65/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3265 - acc: 0.8924 - val_loss: 0.2200 - val_acc: 0.9282\n",
            "Epoch 66/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3385 - acc: 0.8927 - val_loss: 0.2175 - val_acc: 0.9306\n",
            "Epoch 67/200\n",
            "744/744 [==============================] - 171s 230ms/step - loss: 0.3266 - acc: 0.8937 - val_loss: 0.2188 - val_acc: 0.9290\n",
            "Epoch 68/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3115 - acc: 0.9020 - val_loss: 0.2153 - val_acc: 0.9316\n",
            "Epoch 69/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3183 - acc: 0.8969 - val_loss: 0.2161 - val_acc: 0.9278\n",
            "Epoch 70/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3238 - acc: 0.8953 - val_loss: 0.2144 - val_acc: 0.9302\n",
            "Epoch 71/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3246 - acc: 0.8955 - val_loss: 0.2136 - val_acc: 0.9292\n",
            "Epoch 72/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.3186 - acc: 0.8986 - val_loss: 0.2123 - val_acc: 0.9320\n",
            "Epoch 73/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3219 - acc: 0.8963 - val_loss: 0.2082 - val_acc: 0.9324\n",
            "Epoch 74/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3082 - acc: 0.8991 - val_loss: 0.2071 - val_acc: 0.9316\n",
            "Epoch 75/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.3003 - acc: 0.9032 - val_loss: 0.2101 - val_acc: 0.9302\n",
            "Epoch 76/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3135 - acc: 0.8969 - val_loss: 0.2059 - val_acc: 0.9318\n",
            "Epoch 77/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3037 - acc: 0.9033 - val_loss: 0.2046 - val_acc: 0.9335\n",
            "Epoch 78/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2942 - acc: 0.9063 - val_loss: 0.2054 - val_acc: 0.9339\n",
            "Epoch 79/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3036 - acc: 0.9017 - val_loss: 0.2044 - val_acc: 0.9341\n",
            "Epoch 80/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2946 - acc: 0.9045 - val_loss: 0.2031 - val_acc: 0.9320\n",
            "Epoch 81/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2916 - acc: 0.9070 - val_loss: 0.2019 - val_acc: 0.9324\n",
            "Epoch 82/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2888 - acc: 0.9052 - val_loss: 0.2016 - val_acc: 0.9329\n",
            "Epoch 83/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.3004 - acc: 0.9014 - val_loss: 0.1999 - val_acc: 0.9339\n",
            "Epoch 84/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2955 - acc: 0.9034 - val_loss: 0.1987 - val_acc: 0.9333\n",
            "Epoch 85/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2842 - acc: 0.9050 - val_loss: 0.1985 - val_acc: 0.9341\n",
            "Epoch 86/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2876 - acc: 0.9083 - val_loss: 0.1959 - val_acc: 0.9349\n",
            "Epoch 87/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2972 - acc: 0.9058 - val_loss: 0.1968 - val_acc: 0.9341\n",
            "Epoch 88/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2895 - acc: 0.9083 - val_loss: 0.1957 - val_acc: 0.9339\n",
            "Epoch 89/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2708 - acc: 0.9103 - val_loss: 0.1953 - val_acc: 0.9359\n",
            "Epoch 90/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2676 - acc: 0.9130 - val_loss: 0.1961 - val_acc: 0.9347\n",
            "Epoch 91/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2750 - acc: 0.9101 - val_loss: 0.1935 - val_acc: 0.9345\n",
            "Epoch 92/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2765 - acc: 0.9068 - val_loss: 0.1940 - val_acc: 0.9343\n",
            "Epoch 93/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2636 - acc: 0.9120 - val_loss: 0.1926 - val_acc: 0.9353\n",
            "Epoch 94/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2672 - acc: 0.9142 - val_loss: 0.1902 - val_acc: 0.9373\n",
            "Epoch 95/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2561 - acc: 0.9137 - val_loss: 0.1931 - val_acc: 0.9345\n",
            "Epoch 96/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2671 - acc: 0.9136 - val_loss: 0.1926 - val_acc: 0.9357\n",
            "Epoch 97/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2795 - acc: 0.9111 - val_loss: 0.1888 - val_acc: 0.9365\n",
            "Epoch 98/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2639 - acc: 0.9144 - val_loss: 0.1880 - val_acc: 0.9367\n",
            "Epoch 99/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2496 - acc: 0.9180 - val_loss: 0.1877 - val_acc: 0.9367\n",
            "Epoch 100/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2616 - acc: 0.9135 - val_loss: 0.1848 - val_acc: 0.9373\n",
            "Epoch 101/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2507 - acc: 0.9149 - val_loss: 0.1894 - val_acc: 0.9363\n",
            "Epoch 102/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2641 - acc: 0.9140 - val_loss: 0.1867 - val_acc: 0.9355\n",
            "Epoch 103/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2566 - acc: 0.9153 - val_loss: 0.1861 - val_acc: 0.9353\n",
            "Epoch 104/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2548 - acc: 0.9180 - val_loss: 0.1847 - val_acc: 0.9380\n",
            "Epoch 105/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2485 - acc: 0.9190 - val_loss: 0.1846 - val_acc: 0.9355\n",
            "Epoch 106/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2480 - acc: 0.9182 - val_loss: 0.1809 - val_acc: 0.9378\n",
            "Epoch 107/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2462 - acc: 0.9189 - val_loss: 0.1845 - val_acc: 0.9365\n",
            "Epoch 108/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2473 - acc: 0.9176 - val_loss: 0.1818 - val_acc: 0.9384\n",
            "Epoch 109/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2374 - acc: 0.9211 - val_loss: 0.1842 - val_acc: 0.9380\n",
            "Epoch 110/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2450 - acc: 0.9233 - val_loss: 0.1814 - val_acc: 0.9373\n",
            "Epoch 111/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2405 - acc: 0.9204 - val_loss: 0.1810 - val_acc: 0.9373\n",
            "Epoch 112/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2423 - acc: 0.9213 - val_loss: 0.1791 - val_acc: 0.9414\n",
            "Epoch 113/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2512 - acc: 0.9191 - val_loss: 0.1787 - val_acc: 0.9410\n",
            "Epoch 114/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2415 - acc: 0.9206 - val_loss: 0.1802 - val_acc: 0.9373\n",
            "Epoch 115/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2388 - acc: 0.9211 - val_loss: 0.1776 - val_acc: 0.9406\n",
            "Epoch 116/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2394 - acc: 0.9222 - val_loss: 0.1805 - val_acc: 0.9402\n",
            "Epoch 117/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2401 - acc: 0.9210 - val_loss: 0.1772 - val_acc: 0.9400\n",
            "Epoch 118/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2384 - acc: 0.9224 - val_loss: 0.1789 - val_acc: 0.9398\n",
            "Epoch 119/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2405 - acc: 0.9218 - val_loss: 0.1751 - val_acc: 0.9424\n",
            "Epoch 120/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2409 - acc: 0.9246 - val_loss: 0.1760 - val_acc: 0.9406\n",
            "Epoch 121/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2358 - acc: 0.9234 - val_loss: 0.1743 - val_acc: 0.9406\n",
            "Epoch 122/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2328 - acc: 0.9237 - val_loss: 0.1783 - val_acc: 0.9382\n",
            "Epoch 123/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2362 - acc: 0.9234 - val_loss: 0.1744 - val_acc: 0.9412\n",
            "Epoch 124/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2306 - acc: 0.9244 - val_loss: 0.1740 - val_acc: 0.9416\n",
            "Epoch 125/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2286 - acc: 0.9246 - val_loss: 0.1774 - val_acc: 0.9388\n",
            "Epoch 126/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2317 - acc: 0.9239 - val_loss: 0.1719 - val_acc: 0.9404\n",
            "Epoch 127/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2292 - acc: 0.9247 - val_loss: 0.1722 - val_acc: 0.9412\n",
            "Epoch 128/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2247 - acc: 0.9274 - val_loss: 0.1716 - val_acc: 0.9416\n",
            "Epoch 129/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2287 - acc: 0.9262 - val_loss: 0.1724 - val_acc: 0.9390\n",
            "Epoch 130/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2261 - acc: 0.9266 - val_loss: 0.1705 - val_acc: 0.9410\n",
            "Epoch 131/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2294 - acc: 0.9245 - val_loss: 0.1686 - val_acc: 0.9433\n",
            "Epoch 132/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2288 - acc: 0.9268 - val_loss: 0.1705 - val_acc: 0.9420\n",
            "Epoch 133/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2149 - acc: 0.9274 - val_loss: 0.1697 - val_acc: 0.9422\n",
            "Epoch 134/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2242 - acc: 0.9266 - val_loss: 0.1700 - val_acc: 0.9424\n",
            "Epoch 135/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2292 - acc: 0.9252 - val_loss: 0.1699 - val_acc: 0.9408\n",
            "Epoch 136/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2244 - acc: 0.9268 - val_loss: 0.1701 - val_acc: 0.9435\n",
            "Epoch 137/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2285 - acc: 0.9246 - val_loss: 0.1679 - val_acc: 0.9414\n",
            "Epoch 138/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2146 - acc: 0.9302 - val_loss: 0.1681 - val_acc: 0.9433\n",
            "Epoch 139/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2157 - acc: 0.9308 - val_loss: 0.1676 - val_acc: 0.9429\n",
            "Epoch 140/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2146 - acc: 0.9301 - val_loss: 0.1688 - val_acc: 0.9416\n",
            "Epoch 141/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2208 - acc: 0.9285 - val_loss: 0.1664 - val_acc: 0.9429\n",
            "Epoch 142/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2176 - acc: 0.9331 - val_loss: 0.1659 - val_acc: 0.9433\n",
            "Epoch 143/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2127 - acc: 0.9282 - val_loss: 0.1660 - val_acc: 0.9420\n",
            "Epoch 144/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2218 - acc: 0.9257 - val_loss: 0.1643 - val_acc: 0.9447\n",
            "Epoch 145/200\n",
            "744/744 [==============================] - 170s 228ms/step - loss: 0.2176 - acc: 0.9301 - val_loss: 0.1634 - val_acc: 0.9433\n",
            "Epoch 146/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2231 - acc: 0.9273 - val_loss: 0.1686 - val_acc: 0.9418\n",
            "Epoch 147/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2194 - acc: 0.9278 - val_loss: 0.1653 - val_acc: 0.9425\n",
            "Epoch 148/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2132 - acc: 0.9300 - val_loss: 0.1636 - val_acc: 0.9451\n",
            "Epoch 149/200\n",
            "744/744 [==============================] - 170s 229ms/step - loss: 0.2157 - acc: 0.9273 - val_loss: 0.1642 - val_acc: 0.9437\n",
            "Epoch 150/200\n",
            "744/744 [==============================] - 171s 230ms/step - loss: 0.2159 - acc: 0.9274 - val_loss: 0.1626 - val_acc: 0.9457\n",
            "Epoch 151/200\n",
            "744/744 [==============================] - 173s 233ms/step - loss: 0.2052 - acc: 0.9304 - val_loss: 0.1619 - val_acc: 0.9447\n",
            "Epoch 152/200\n",
            "744/744 [==============================] - 172s 232ms/step - loss: 0.2105 - acc: 0.9300 - val_loss: 0.1637 - val_acc: 0.9439\n",
            "Epoch 153/200\n",
            "744/744 [==============================] - 173s 233ms/step - loss: 0.2030 - acc: 0.9338 - val_loss: 0.1623 - val_acc: 0.9439\n",
            "Epoch 154/200\n",
            "744/744 [==============================] - 173s 233ms/step - loss: 0.2021 - acc: 0.9336 - val_loss: 0.1628 - val_acc: 0.9435\n",
            "Epoch 155/200\n",
            "744/744 [==============================] - 173s 233ms/step - loss: 0.2171 - acc: 0.9254 - val_loss: 0.1629 - val_acc: 0.9431\n",
            "Epoch 156/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2110 - acc: 0.9310 - val_loss: 0.1654 - val_acc: 0.9435\n",
            "Epoch 157/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2064 - acc: 0.9344 - val_loss: 0.1626 - val_acc: 0.9447\n",
            "Epoch 158/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1970 - acc: 0.9348 - val_loss: 0.1621 - val_acc: 0.9441\n",
            "Epoch 159/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2136 - acc: 0.9286 - val_loss: 0.1634 - val_acc: 0.9453\n",
            "Epoch 160/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2004 - acc: 0.9332 - val_loss: 0.1602 - val_acc: 0.9441\n",
            "Epoch 161/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2097 - acc: 0.9325 - val_loss: 0.1624 - val_acc: 0.9431\n",
            "Epoch 162/200\n",
            "744/744 [==============================] - 174s 233ms/step - loss: 0.2188 - acc: 0.9271 - val_loss: 0.1631 - val_acc: 0.9439\n",
            "Epoch 163/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2041 - acc: 0.9322 - val_loss: 0.1633 - val_acc: 0.9429\n",
            "Epoch 164/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2098 - acc: 0.9318 - val_loss: 0.1586 - val_acc: 0.9447\n",
            "Epoch 165/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2067 - acc: 0.9339 - val_loss: 0.1598 - val_acc: 0.9439\n",
            "Epoch 166/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1976 - acc: 0.9338 - val_loss: 0.1589 - val_acc: 0.9457\n",
            "Epoch 167/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1997 - acc: 0.9345 - val_loss: 0.1598 - val_acc: 0.9445\n",
            "Epoch 168/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1998 - acc: 0.9334 - val_loss: 0.1595 - val_acc: 0.9449\n",
            "Epoch 169/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2001 - acc: 0.9342 - val_loss: 0.1580 - val_acc: 0.9449\n",
            "Epoch 170/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2024 - acc: 0.9351 - val_loss: 0.1597 - val_acc: 0.9453\n",
            "Epoch 171/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2024 - acc: 0.9350 - val_loss: 0.1576 - val_acc: 0.9461\n",
            "Epoch 172/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.2035 - acc: 0.9311 - val_loss: 0.1578 - val_acc: 0.9449\n",
            "Epoch 173/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1914 - acc: 0.9347 - val_loss: 0.1591 - val_acc: 0.9441\n",
            "Epoch 174/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1866 - acc: 0.9374 - val_loss: 0.1589 - val_acc: 0.9459\n",
            "Epoch 175/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1992 - acc: 0.9350 - val_loss: 0.1589 - val_acc: 0.9447\n",
            "Epoch 176/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1888 - acc: 0.9373 - val_loss: 0.1596 - val_acc: 0.9455\n",
            "Epoch 177/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1997 - acc: 0.9327 - val_loss: 0.1562 - val_acc: 0.9465\n",
            "Epoch 178/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1961 - acc: 0.9373 - val_loss: 0.1549 - val_acc: 0.9457\n",
            "Epoch 179/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1909 - acc: 0.9393 - val_loss: 0.1558 - val_acc: 0.9459\n",
            "Epoch 180/200\n",
            "744/744 [==============================] - 174s 233ms/step - loss: 0.1994 - acc: 0.9372 - val_loss: 0.1550 - val_acc: 0.9469\n",
            "Epoch 181/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1991 - acc: 0.9339 - val_loss: 0.1551 - val_acc: 0.9476\n",
            "Epoch 182/200\n",
            "744/744 [==============================] - 174s 233ms/step - loss: 0.1899 - acc: 0.9350 - val_loss: 0.1561 - val_acc: 0.9467\n",
            "Epoch 183/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1924 - acc: 0.9391 - val_loss: 0.1552 - val_acc: 0.9457\n",
            "Epoch 184/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1870 - acc: 0.9360 - val_loss: 0.1542 - val_acc: 0.9480\n",
            "Epoch 185/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1938 - acc: 0.9361 - val_loss: 0.1539 - val_acc: 0.9473\n",
            "Epoch 186/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1953 - acc: 0.9362 - val_loss: 0.1517 - val_acc: 0.9496\n",
            "Epoch 187/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1933 - acc: 0.9368 - val_loss: 0.1527 - val_acc: 0.9469\n",
            "Epoch 188/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1857 - acc: 0.9389 - val_loss: 0.1522 - val_acc: 0.9482\n",
            "Epoch 189/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1934 - acc: 0.9342 - val_loss: 0.1550 - val_acc: 0.9478\n",
            "Epoch 190/200\n",
            "744/744 [==============================] - 174s 233ms/step - loss: 0.1889 - acc: 0.9394 - val_loss: 0.1557 - val_acc: 0.9473\n",
            "Epoch 191/200\n",
            "744/744 [==============================] - 174s 233ms/step - loss: 0.1977 - acc: 0.9360 - val_loss: 0.1526 - val_acc: 0.9471\n",
            "Epoch 192/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1937 - acc: 0.9364 - val_loss: 0.1533 - val_acc: 0.9473\n",
            "Epoch 193/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1907 - acc: 0.9351 - val_loss: 0.1542 - val_acc: 0.9463\n",
            "Epoch 194/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1908 - acc: 0.9355 - val_loss: 0.1517 - val_acc: 0.9475\n",
            "Epoch 195/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1832 - acc: 0.9402 - val_loss: 0.1501 - val_acc: 0.9480\n",
            "Epoch 196/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1930 - acc: 0.9358 - val_loss: 0.1527 - val_acc: 0.9478\n",
            "Epoch 197/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1881 - acc: 0.9378 - val_loss: 0.1528 - val_acc: 0.9469\n",
            "Epoch 198/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1972 - acc: 0.9369 - val_loss: 0.1533 - val_acc: 0.9482\n",
            "Epoch 199/200\n",
            "744/744 [==============================] - 177s 238ms/step - loss: 0.1826 - acc: 0.9396 - val_loss: 0.1508 - val_acc: 0.9473\n",
            "Epoch 200/200\n",
            "744/744 [==============================] - 174s 234ms/step - loss: 0.1870 - acc: 0.9398 - val_loss: 0.1516 - val_acc: 0.9467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC9lIjf3clWQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "653351a7-3636-423f-a887-ab8fba1d5242"
      },
      "source": [
        "plt.plot(history.history['acc'], label='accuracy')\n",
        "plt.plot(history.history['val_acc'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
        "print(test_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160/160 - 30s - loss: 0.1445 - acc: 0.9535\n",
            "0.953529417514801\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA7L0lEQVR4nO3deXxU9bn48c+Tfd9DCGEJO2EVRBYVQXFBi1orKtarllu1WrVV7622Wpfe9vbaevu7V2urF+tSrUqtS1WqoihKVVT2fYdAQsi+r7N9f398hxBCEpKQmQmZ5/168crMmTNnnpwJ5znfXYwxKKWUCl4hgQ5AKaVUYGkiUEqpIKeJQCmlgpwmAqWUCnKaCJRSKshpIlBKqSDns0QgIs+JSLGIbGnndRGRJ0Rkj4hsEpEpvopFKaVU+3xZIngBmNfB6xcDI73/bgGe8mEsSiml2uGzRGCMWQmUd7DL5cCLxvoKSBKRTF/Fo5RSqm1hAfzsLCCvxfN877bDrXcUkVuwpQZiY2NPHzNmjF8CVEqpvmLt2rWlxpj0tl4LZCKQNra1Od+FMWYxsBhg6tSpZs2aNb6MSyml+hwROdDea4HsNZQPDGrxfCBQEKBYlFIqaAUyEbwD3ODtPTQDqDLGHFctpJRSyrd8VjUkIq8Cc4A0EckHHgbCAYwxTwPvAZcAe4B6YJGvYlFKKdU+nyUCY8y1J3jdALf76vOVUkp1jo4sVkqpIKeJQCmlgpwmAqWUCnKaCJRSKshpIlBKqSCniUAppYKcJgKllApymgiUUirIaSJQSqkgp4lAKaWCnCYCpZQKcpoIlFIqyGkiUEqpIKeJQCmlgpwmAqWUCnKaCJRSqidVHYJ1L4HLEehIOi2Qi9crpVTv1lABkYkQ4r1nrimE3M8hfzUkDYER50PqCFj7PGx/F5KzYcsb0FQN616EAafB3hUQkwIJWZAwABIHQlgUuB2QdTogsPMfULQV6ssgKtFuD4+GrX+37xl6DvQbCxnjITa1x39NsQuFnTqmTp1q1qxZE+gwlFLtqS2GqjwYMAVEjm4v2gYHV8HIC2DHe7BvBQyYDNlnQ9ZUCIs8dn8AY6BgHTRUQnx/2PsJOOph1IWQedqx+3s8cOALiEoAZyN8/TTEptnj15eDqwncTVBdAIWboSIXhs62x4rtB9/8HzTVwLA5ULrbXvAr9kNUkr3AN5RD5UH7WWFR4Gq0j6MSobEKkofaYw+aBuO+DR8+BB6XPZ6zHqoP2dKCu+n4cxYSBmmjbbwN5TYpGI89L3UlUHnA7jfzDrjoP7v1tYjIWmPM1DZf00Sg1CmqeDtExELS4O4fwxhY92c48CXkXGqP1VgNjlroPwHCY+Af/2bvaKfdAqv/BFX59m547GUgobDt7/aiGp1s72Q//g9orIT0MXDGTTDkTNi1DD79L3sXfETSYKjMA1pcgyIT7B1wwgB7ISzZBTUFrYIW+55479110VZIGwFuJxRvO7pbdDI4G45esI+IiIP00fa9ez+Fpiq7PSoRYtKgfK99PORsyJpiL8LVh22CyTzNJpb+E+2Ffe/H9txlz4IpN9jzeaT00Fhln0cnHXu+68tsUpIQOPgluF0w6qJj96svt99B0mD7nppCKNlhk2G/nK5+y/asaSJQ6hTx5ZOw5jmIz4TZP4Hsc2y1Q22xvVuMTbN3qHlfw2e/sReTsd+GuAwYMhNGXgQfPWSrGppqYPZ9MH4BbFoCB1bZC2V9OUy5HiZeDR8+CPs/g/BYcNYdG4uE2otTY7W9KBs3hIRD2kh74XfW2/3CoiFlqL3bbaqCjAkw9Xuw/i9QsP7o8UZeZOPZ/6m9kI443yaMA6ugaAt43PZ5Vb69yEqIvRCOvMhetKvyYPAMmyx2fwg7/mEvqv3G2qTorIPpt0FouL2ITrjqaDKJz7BJLSQMIuOPliRcDls6KN9nSwaRCfaiG9cPQkJ9+1130Ya8Soalx5IQFd6t92siUOpkOOohIqb9190uewGLSoLQVs1uuV9AeJS9Uy7dYy+U/SfChlfsnWzCAMicBGmjbLXHO3fYfetKoLYEhs2GXR+0/bkTroLoFNj8mq0KcTXYu+SaAhgz3yaC/Z/ZC7px27rs/hPsxXHb2/YYUYkw92F7N5v7ub24R8bbqo/t78DBr+HiRyE0wlbnTFgAqcPBUWfrxI0Hci6DyDh7ngrW2SqhI+crfy2U7rJVQOmjj6/6OYU43R7CQ7vWv+brfWW8/PVB7r8kh+jwUJ79fB/5FQ2MyYzn+2cP41BFAwfL68lOi8HlNqw7WMHHO4oZ2S+OnMwE9pXUMSYznqKqRh58ewtXTx3Ef14xoVvxayJQ6ghjbHXBkQuVMbDzfdsoOPpiWwVyRFU+fPEErHnW3nWf/zDsX2nvYEu227vR+oqjVQsRcTB4pq06SBsJed/AF/9r72zHfMteSI0bIuLBUdP2XfiQs+H6t2xj4/MX24vonPth1j32Tr6+1FY5hEV6Gxq93C5Y9Xv45hk4/xcw8SpbZ/71U/bufdotNqYjdn4Ah9bAjB8e+zv3QRV1Dj7YWkhdk4v+iVHMHpVObmk9K3eXsOisbGIibPLeXVTD7uJaQkQ4UFZHTGQYF+Rk8NqaPN7dWMDu4lpmDkvlmjMG4TGG3NI6dhfXsq+kjsGpMWSnxvDRtiL6xUdx0fj+FFU38tzn+3F5DKMz4gkNEXYUVpMeH0lRdRMj+8Wxr7QOt+fYa3BaXARldQ5aX5pnjUzj99dOJikmolvnQROB6nvqy2HTa7b+tq7EXhzHzIfJ1wPGXsQLN8G+z+zdbGwqFGywvT1qDtvGv9SRUFcMhzcePW5EvL0jDgmDqoP2Ij7ifNiz3N79gr0Lz5wIsen2cUyKLQ2U7rJ31aU7jx5v8r/YC/LGV2DiQltfvu9TGH+lTQ6NlXBora0rD42w9e6R8fa9daW21DBstj/OaK/mcHkormkkKymavPIG7nltA2My41l4xmDGZyVyqLKBbQXVnDs6HRFh5a4Slqw+yO7iWvLK63G6j17nIsJCcLjsd3nm8FRuP3cEL606wAdbC9v9/LNGpJLTP4G3NxZQUmMbe0MEhqTGkp0aw47CGoqqGzlrRBr5FQ3sL7UJ/qJxGVwxeSB3vrqOEBH+7/rTmTO6H39bk8djy3ZyyYRM5ub0I6+8gajwELLTYpk8KInimibyKxoYlhbLN7nlVNU7+c6ULMK6WCJpSROB6n0aKuyFLzQCvvqj7TExep59zRjbmLb8YVstUl9uq06GnwujLoatb8LaF2w9cEScrTeXEFvPG9vPHtvjtMcKj7UXcFeDbeAcNM1WkRRtsXXaxm0bNDPG24a/+nJ7N+5ssA2DY75lq0JyP/f2eLnQVu10VMVRWwLV+TaZ9J9wdFtcui/PaK/n8RhqmlwkRofT6HSzv7SO0RnxeIxhfV4lK3YU88XeMgqrGvj5t8YyY1gqb67L5+PtxWzIq8Th9nDJhP7sLKyhsKoRtzG43Ia7LxjFC1/mUlLTRP+EKOocLmoaXaTHR3JGdjJDUmOZPzGTgckx7CmuYemmw6THR5IUHcEDf9+MMRAfFcaiM7OZNz4TjzEMSo7hYHk9H20rZM6YfkwZnAzQHHdkWAgDkqKJCrftCMYYGp0eoiNCMcZQVN1Ecmw4kWH29bUHKogMC2F8VmLAzr8mAhV4xthGOWe97eL36aO2kTExyzZgSijMfQg2vmovxlEJULbH3uXH94fCLZD/jb2oS4i9oz77bsgYd/T4G5fYO/fEgZAyzCaPrNPtBdlRY+vD1UlZuauEj7YVUVLTRE5mAlUNTr7YU0pmUhSpsZFUNzrJyUwgISqM9QcrSYgO4/QhKXz7tAHc/dpGPthymOumD+GzXSXsL60jMzGKeoebqgYnYSHCaYOSaHJ52HyoivBQwek2jBuQwNkj0ggNERav3IcIvPT96eT0T+CHr6zliz1lZCREcs8Fo/hoWxFpcZHMHpXO3JwMIsI6voNevq2I6kYnF4/PJDqidzUO9zRNBKpnedy2KsbZAIc32Dpot8NenGNSbUMo2GqPXcvshbihwnZ/OyLnUnvxL9wE59wL3yy2DY2Jg2HQGbYf95yfwZhLjr6nusBe6LPPthd61SVVDU425FVyuLKBM4enUd3o5IG3NnPxhEx+cM4wjIElq/NYc6CcRWcOZUdhNR9vL+buC0Yxun88r6/N5yevbyQuIoy0+Ehyy+oIDwlh+rAUSmqaqG5wEh0Ryv7SOjwGspKiqXe4qKh3MjA5mvyKBs7ITmZ1bgVZSdF8/+yhfLWvjPiocM4b049Zo9JIiArH4fLw3x/uxOHycMPMIQxLj2v+HbYWVNHgcDM127ZrOFweXv3mIHNz+jEwuYMGfaWJQPWgg1/B27fbu/XWwqJt3+2W/b6zptoeKGB7nCQNttU5g6YdW73SUGFHUU5YcLSOXJ1QRZ2Dt9Yf4tJJA0iPjyS/op7nPs+lqLqRwakxVNQ5qG1y0ej08M/dJTR568ZDQ4TQECFEoNHpYeawVIprGtlbUkdEaAgOt90vLEQICRGGpsayq7iGs4an8cwNU4mOCKW60UmICHGRx/aUqqp3Uu90kZkYjTGGJavz+OXSbSw8YzAPzs9hb4ktCcS2ep/yLU0EqnvcLvjkP2D1c3aIu7PeNnQmDYJpP7DdIjPG226BjdXw4c9tX++Lfu29Yxfbf1t1aN3BCoqrm5g3vv8x23cUVvPPXaWMz0qk0eXG6fJwfk4GH24r5Pkvcrl1znD+96NdbMyvIjYilEEpMewqqiE0RMhMjOZQZQPJMREkRIVhgLNHpHHx+P6kx0fy2po8Dlc18vCl43jpqwP8ff0hhqTG8J0pWZw3JoO/fHWAgcnRnDk8jd98sIPyOgeTBibxg9nDmuvFu8Lh8pywmkb5liYC1bbi7baqZfpt9u682Nslct+ntnG05rC9sI+4wFYBhUXZXjAzb9e79h7Q4HDz22U7eOHLXIyB2+YMJzo8lNzSOgamxLB45V4anZ5j3jOmf3zzxd7pNoQI/OKycaw5UEFFvZPJg5K45oxBDEiyd+NyCvfbVz2ro0SgZbNg43Hb3jZh0fDajbar46G1UFNkh7uDrbsfNM0Orz//EVtdo05KRZ2D8noHX+wpZcWOYrKSo/l8dym5ZfXcMHMIDQ43T326F4CU2AjK6xxMy07h0SsncKCsnpiIUPIqGvjVP7Zxzqh0fnfVJF74MpfR/eOZP3EA18/MPu4zNQmoztISQV/lqIPiHbYbZdleO/Jz+FxYci3kr7ENrrs/hHFXwNa3bL393IdsNU/GBJ/McHiqqne4iA4PpbLeycPvbOXsEWlcNXUgAJvyq9haUE1STDiDU2IYlh6LIBRVN7K/tI69JbV8sKWQNQcqmo+XnRpDaa2DlNgIHr1yAmcOT8MYw7KtReRkxjM4JYbC6kb6xUcRGnLsxdzp9hAWInqRV12mVUPBproAXviW7VffUkS8LQ1knW5HleZcBte8BLs+hPRRdpCVavbZrhJ+9+FONuVXMXVIMrVNLnYU1gCQk5lASU0jpbUnnnN+WHosV5yWxaCUGEZmxDFuQCIej0FE79qV/2jVUF/ldh0/t01NEbww3w5guuL/7GyKydm2jv/L39v6/QlX2akSsqbY94y60N+R9wout4c1BypIi4tEBPYU11LT6GJoWgwZCVHc8fI6UuMiuHnWUN7eUEB1o5MX/3Ua2w9X89G2IsaO6seMYSnMGJZKbZOLfSV1HCyvRwRSYyMYlh7HsLRYkmOPnxIgJEQTgOo9tERwqtr6d3jrVjs3eViknRNn6r/Chpdt187r/w6Dpwc6yl6nyeVmd1EtWUnR/PTNTSzbWtTmfmlxkTQ63bz/41kMSomh0emmutFJv/goP0esVM/QEkFf46iDD35mp0f4xz12W1wGfHCfHbx17V+DOgm07t/+7sYC7n9rM8PS48gtraOqwdm8708uGk3/hCgMMCojjsTocN5Yd4g//XMf/3nFeAal2EFKUeGh3eo2qdSpwKeJQETmAY8DocCfjDGPtno9GXgOGA40Av9qjNniy5hOWQUb7ORqEXH2rr+mABa9753mOBpm3AY737Nzro+YG+hofcLtMXy0rZAzR6QRGRbCS6sO4PYYxmTaKQgEeOmrA/z6ve00uTyM7BfHI5eN46G3t5AeH0l0eAjnjk5nzuh+5JbVMWlgEueO6Xfc59xzwSjumjtSq29U0PBZIhCRUOAPwAVAPrBaRN4xxrRYQoj7gQ3GmCtEZIx3/755FTsZXy+G9+/lmJWcptxgZ7IccubRbTmX+j00f2l0urlryQY+2FrIyH5xZCRE8fme0ubXMxIiaXJ5qKx3Mmd0Omdkp/Diqlyu+9PXhIcKf/3BTEZldH7sgyYBFUx8WSKYBuwxxuwDEJElwOVAy0QwFvgvAGPMDhHJFpEMY0zbFbfBxtkAyx6w8+GP/hac9SO72EjKsKCZa6fJ5eb9zYU8uWIPe4pr+d6Z2by1/hB7S2r57YKJzBvfny/3lPLW+kMkx0Rw1og05k/MRET4zpQs/v1vGzk/J6NLSUCpYOPLRJAF5LV4ng+0rrjeCHwH+FxEpgFDgIHAMYlARG4BbgEYPPgk1mftzRqrYPtSO7irusAuQFKRa6uDzrzTLjbSy5bO6yluj6Gm0YnLY0iNtYtyrNhRzNf7y1m+vYjKeicj+8Xx7I1TmZuTwc3nDKO0polJg5IAmDc+k3njM487bmZiNC/fNMPPv41Spx5fJoK2ytatuyg9CjwuIhuAzcB6wHXcm4xZDCwG22uoZ8PsBRoqbJfPoi12zdSkIXZA17A5cNp1dh7+PmjdwQqe+3w/n+0soabJfu1xkWHUO1x4DCTFhDNrZDpXTsninJHpzdU1WUnRZCVFBzJ0pfoUXyaCfGBQi+cDgYKWOxhjqoFFAGJH1uz3/gseTbXw8lV2datrl9iFukP61uRc2w9Xszq3nPDQEMrrHBRUNrCvpI5V+8pIiY3gkgmZjO4fT4jA/tI6EmMiuGhcBjn9E7SuXik/8GUiWA2MFJGhwCFgIfDdljuISBJQb4xxADcBK73JoW9zNtpVufrl2J+H1sHVL9o1c/uIIz18nv8il6/3lx/zWlJMOFlJ0dx1/khunjVMpyNWKsB89j/QGOMSkTuAZdjuo88ZY7aKyK3e158GcoAXRcSNbUT+vq/i6TU8bnjzJtj+7tFtVyyGnPmBi6kHVDU4SYwOB+zUDL9auo3dxXbg1v2XjGH+xAEAJEaH64VfqV7Gp/8jjTHvAe+12vZ0i8ergJG+jKHX+fBBmwQu/JVdpzfEuzj6KWp3UQ2/XbaTj7YVcflpA+ifGMX/fbaPIakxPPndyVw8PvO4idOUUr2L3pr5065l8NUfYNottifQKa6kponr/vQ1jU43l582gHc3FuAxcO20wTxy2djmhbuVUr2bJgJ/Kd0Df/+hneL5wl8FOpoucbo93PfGJmobXVwxOYui6kbqHG4+21lCVYOTv99+FjmZCdx4ZjaHKxu5ZEJ/nVVTqVOIJgJfczbCzn/A0nvsOIAFz9pJ4k4Rbo/hntc28u7GAhKjw/lw29EhHiECj145kZzMBACmDE6GPjrMQ6m+TBOBL+Wvgb98xw4W6zcWrn2118/5b4zh1W/y+NvaPH5y4WheW5PHuxsLuG/eGBadlc3GvEqy02JJiAqnyeUmKeb4KZaVUqcWTQS+4myw00RHxMOC5+3i76HhgY6qQ3uKa/n9J7t5e0MB0eGhfPdPXwN2hs7b5gwHYPqwoyuXRUdoG4BSfYEmAl9w1MP7P4Gy3XD9WzD8vEBH1CGn28Ov39vO81/kEhEWwo/mjuSmWUN5fPluRvaLY+E0re9Rqi/TRNDT8tfCX//FThN95p29PgnUNrn4/gur+Xp/OTfMHMKP5o4kLc62YTw4f2yAo1NK+YMmgp5UuMW2CUQl2rUCWk4R3YsYY1iyOg8B3liXz7qDlfzPNZO4YvLAQIemlAoATQQ9xeOGv15nF4a58V1IHhLoiI6z/mAFOZkJbMir5GdvbgYgNER4YuFkvjXx+Nk7lVLBQRNBT9n5vp02+uqXemUSeH1tPv/+t418a2ImNY0u0uIieOXmGUSFhTI4NSbQ4SmlAkgTQU/5+mlIGAijLwl0JMfZVlDNz/++mbS4SP6x6TAA984brYu1KKUATQQ9o2gb5P4T5j4MoYE/pR6P4Q8r9nCgvJ7pQ1P41T+2kxAVztI7z+beNzaxKb+Kf5nR+0otSqnACPxV61RnjF1PODIBptwY6GioqHNw7xub+GhbERGhIby+Nt+7utcZ9EuI4tkbz6Cm0UlCVO8e06CU8h9NBCdr7Qu2NHDp43ZVsQAxxvD62nx+/d52qhtdPDR/LAumDuTTnSXMGZ3efOEPDREdDayUOoYmgpNRdchOKz30nICVBjwewz/3lPKHT/bwTW45U4ck86srxjOmv53/57JJAwISl1Lq1KGJoLuMgaV3g8cFlz4BAZht82BZPbe9vJatBdWkxUXw2ysnsuD0gbq8o1KqSzQRdNe2t2H3Mrjo15Ay1K8fXdfkYsnqPJ74eDfGGP7f1ZOYP3EAEWF9a61jpZR/aCLorlVPQuoImH6rXz92X0kt1z/7DYcqG5gxLIXfXDmRIamxfo1BKdW3aCLojoINkL8a5j1q1xjwg6oGJ+9vPsxjy3YC8NdbZhwzE6hSSnWXJoLuWP0nO5XEpGv98nHldQ4uefyfFFY3Mjojnj/+yxSGp8f55bOVUn2fJoKuqjoEm/8GE6+B6CSff5wxhp+9uYnyOgev3DSdmcNTdRlIpVSP0tbFrvrkl7bH0Kx/88vHvbHuEMu2FvFvF47izBFpmgSUUj1OE0FXFKyHja/CjNv8MrFcXnk9j7yzlWlDU7hp1jCff55SKjhp1VBXfP6/EJUEs+7x6ce4PYZPdhTz+Me7APjdVZMI1bEBSikf0UTQWTWFsGOp7S4alejTj/rdhzv546d7SY4J57EFExmUotNEK6V8RxNBZ61/yY4invqvPv2YmkYnL606wLxx/fn9dycTHqq1d0op39JE0BkeD6z9MwybA6nDffIRxhjqHG7+ujqPmiYXPzx3uCYBpZRfaCLojKLNUJUH5z7gk8M73R5u+8talm8vJixEmDY0hYkDk3zyWUop1Zomgs7Yv9L+HDa7xw9tjOG+NzaxfHsx350+mKp6JzfN8u/cRUqp4KaJoDP2fQapIyGh56d0XnewkjfXHeLO80bwbxeO7vHjK6XUiWgl9Im4HHDgS5+UBsAuKh8dHsoPZvum7UEppU5EE8GJFKwDZ51dfKaHVNY7WLh4FW9vOMTSjQVcPL4/cZFaOFNKBYZefU5k32eAQPasHjvkip3FfLWvnK/2lQOw4PSBPXZspZTqKk0EJ7LzPciaAjEpPXbIf+4uJTkmnOHpcVQ1OJmh00krpQLIp1VDIjJPRHaKyB4R+WkbryeKyLsislFEtorIIl/G02VV+XB4A4yZ32OHNMbw+e5SzhqRxt9uncm7d56tS0sqpQLKZ4lAREKBPwAXA2OBa0VkbKvdbge2GWMmAXOA34lIhK9i6rId79mfOZf22CF3F9dSXNPErJF2JtGocP8sbKOUUu3xZYlgGrDHGLPPGOMAlgCXt9rHAPFi51aOA8oBlw9j6pod70LaKEgb2WOH/OfuUgDOHpneY8dUSqmT4ctEkAXktXie793W0pNADlAAbAZ+bIzxtD6QiNwiImtEZE1JSYmv4j1WfTnkftFj1UJNLjePvLOV336wg9EZ8WQlRffIcZVS6mT5MhG0VfFtWj2/CNgADABOA54UkYTj3mTMYmPMVGPM1PR0P91J71oGxt1jieDlrw7ywpe5XDZpAH+6cWqPHFMppXrCCROBiMwXke4kjHxgUIvnA7F3/i0tAt401h5gPzCmG5/V83YshfgBMGDySR+q0enm6c/2MmNYCo9dNUmnlVZK9SqducAvBHaLyG9FJKcLx14NjBSRod4G4IXAO632OQjMBRCRDGA0sK8Ln+EbjnrY8zGM+RaEnHyh6bU1eRTXNPGjuT3X1qCUUj3lhOMIjDH/4q2uuRZ4XkQM8DzwqjGmpoP3uUTkDmAZEAo8Z4zZKiK3el9/Gvgl8IKIbMZWJd1njCk96d/qZO1bAa4GmwhOUmW9g8eX72ZadgozdbyAUqoX6tSAMmNMtYi8AUQDdwFXAD8RkSeMMb/v4H3vAe+12vZ0i8cFwIXdiNu3dn0AkYmQffZJH+o3H+ygssHJI5eN04XnlVK9UmfaCC4VkbeAT4BwYJox5mJgEvDvPo4vMAo22NHEoeHdPoTbY1i8ci+vfpPHojOzGTvguDZwpZTqFTpTIrgK+B9jzMqWG40x9SLi23UbA8HthJIdMP0HJ3WYHy1Zzz82Heb8nAzuuXBUDwWnlFI9rzOJ4GHg8JEnIhINZBhjco0xH/ssskAp3Q1uB/Sf2O1D7Cis5h+bDnPr7OHcN2+0VgkppXq1znSJ+RvQcpCX27utbyrcbH9mjO/2IZ79536iw0O5dfYwTQJKqV6vM4kgzDtFBADex71nPqCeVrQZQiO6Pa1ESU0Tb28oYMHpA0mK6bunSSnVd3QmEZSIyGVHnojI5UDgu3j6SuFm6JfT7YbidzYW4HB7uPHM7J6NSymlfKQzbQS3Ai+LyJPYvv55wA0+jSpQjIHCLTBqXrcP8dG2QkZnxDOiX1wPBqaUUr7TmQFle4EZIhIHSEeDyE55tUVQXwr9u9c+UFHn4Jv95fxwzogeDkwppXynUwPKRORbwDgg6kjjpzHmP3wYV2AUb7c/+7VeNqFzPtlRjMfAheMyejAopZTyrc4MKHsauAa4E1s1dBUwxMdxBUbJTvszfXS33v7RtiL6J0QxISuxB4NSSinf6kxj8ZnGmBuACmPML4CZHDuraN9RuhOiEiGu63f05XUOPtlZzEXjMrTLqFLqlNKZRNDo/VkvIgMAJzDUdyEFUMkuSBsN3biQv7YmD4fLw3Uz+mZhSSnVd3UmEbwrIknAY8A6IBd41YcxBU7Jjm5VC7k9hr98dYDpQ1MYlRHvg8CUUsp3Omws9i5I87ExphJ4Q0SWAlHGmCp/BOdXdWW2x1A3EsEnO4rJr2jgpxf3jjV1lFKqKzosEXjXD/5di+dNfTIJgG0fAEjv2sXcGMMTH+9mYHI0F43r74PAlFLKtzpTNfShiFwpfb0F9EiPobSuzRT60bYiNh+q4kdzRxIe6ssloJVSyjc6M47gHiAWcIlII7YLqTHG9K0J9kt3QXgMJHatQ9STK/aQnRrDdyZn+SgwpZTyrc6MLA6O1s+yvZAyvEtrFFfVO9mUX8VPLhpNmJYGlFKnqBMmAhE5p63trReqOeXVHIaEzC69ZfMh21wyaWCSDwJSSin/6EzV0E9aPI4CpgFrgfN8ElGg1BbBgNO69JZNhyoBdCSxUuqU1pmqoUtbPheRQcBvfRZRILhdUFsMcV3r9bMpr4rs1BgSY7q/trFSSgVadyq284HuL9/VG9WVAAbiuza1xOZDVUzQaiGl1CmuM20EvweM92kIcBqw0Ycx+V9tof3ZhRJBaW0Thyob+J4uQKOUOsV1po1gTYvHLuBVY8wXPoonMGqK7M/4zjcWH2konjhQ2weUUqe2ziSC14FGY4wbQERCRSTGGFPv29D86EiJoAtVQ6v3lxMaIozThmKl1CmuM20EHwPRLZ5HA8t9E06A1HgTQWy/Tr/lkx3FnJGdTFxkp9b2UUqpXqsziSDKGFN75In3cYzvQgqAmkKISYWwiE7tnl9Rz47CGuaO0ZXIlFKnvs4kgjoRmXLkiYicDjT4LqQAqC3qUvvAJzuKAZib0/kShFJK9Vadqde4C/ibiBR4n2dil67sO2oKu7Qq2fLtxQxNi2VYepwPg1JKKf/ozICy1SIyBhiNnXBuhzHG6fPI/Km2CPrldGrXmkYnX+0t44aZuhKZUqpv6Mzi9bcDscaYLcaYzUCciPzQ96H5icdjE0EnSwSf7CjG4fYwb7yuPaCU6hs600Zws3eFMgCMMRXAzT6LyN/qy8Dj6nQbwfubC+kXH8mUwck+DkwppfyjM4kgpOWiNCISCnSue82poAtjCOodLj7dVcxF4/oTEtK31+lRSgWPzjQWLwNeE5GnsVNN3Aq879Oo/KnW9gDqzBiCz3aW0Oj0cLFWCyml+pDOJIL7gFuA27CNxeuxPYf6hvoy+zM2/YS7rthZTEJUGNOGpvg4KKWU8p8TVg15F7D/CtgHTAXmAts7c3ARmSciO0Vkj4j8tI3XfyIiG7z/toiIW0T8e5WtK7E/Y1NPuOuXe8uYOTxVVyNTSvUp7ZYIRGQUsBC4FigD/gpgjDm3Mwf2tiX8AbgAO3X1ahF5xxiz7cg+xpjHgMe8+18K3G2MKe/er9JNdaUQEgZRSR3ulldeT35FAzedPdQ/cSmllJ90dGu7A3v3f6kx5mxjzO8BdxeOPQ3YY4zZZ4xxAEuAyzvY/1rg1S4cv2fUlUBMGkjHjb+r9toqpDNHpPkjKqWU8puOEsGVQCGwQkSeEZG52DaCzsoC8lo8z/duO46IxADzgDfaef0WEVkjImtKSkq6EEIn1Jd1qn1g1b4y0uIiGNlPRxMrpfqWdhOBMeYtY8w1wBjgU+BuIENEnhKRCztx7LaShmljG8ClwBftVQsZYxYbY6YaY6amp5/4ot0ldSUQ2/FdvjGGL/eWMmNYKnKCkoNSSp1qOtNYXGeMedkYMx8YCGwAjmv4bUM+MKjF84FAQTv7LiQQ1ULQqURQUNVIUXWT9hZSSvVJXer+YowpN8b8nzHmvE7svhoYKSJDRSQCe7F/p/VOIpIIzAbe7kosPabuxFVDW72rkY0boIvQKKX6Hp+tqmKMcYnIHdgBaaHAc8aYrSJyq/f1p727XgF8aIyp81Us7XI2gqPGrkXQgW2HqxGBnMx4PwWmlFL+49PltYwx7wHvtdr2dKvnLwAv+DKOdtWX2p8nKhEUVDM0LZaYCF2NTCnV9wT3yKjmwWQdtxFsK6jWaiGlVJ8V5IngxNNLVNY7OFTZwNjMBD8FpZRS/hXkicBbIuigjWDb4WoAxg3QRKCU6puCOxF0oo1gW4FNBGM1ESil+qjgTgR1JRAaAZHt9wbaWVhDWlwkaXGRfgxMKaX8J8gTQaktDXQwWji3rI5habF+DEoppfxLE8EJegzlltUzJDXGTwEppZT/BXciaKyEqPa7hdY2uSipaSJbSwRKqT4suBOBox4i2p9N9ECZHew8VBOBUqoPC+5E4KyD8ParfXJL6wG0akgp1acFdyJw1ENEB4nAWyLITtUSgVKq7wruROCsh/D2L/K5pXX0i48kNlLnGFJK9V3BmwiMAUfdCUsEWhpQSvV1wZsI3A4w7g7bCPaX1pOdpu0DSqm+LXgTgcO7/EFE23f8tU0uSmu166hSqu8L3kTgtD2C2isR5FfY1wcla4lAKdW3BW8icHgTQTslgvzyBgAGJkf7KyKllAqI4E0ETm/VUDslgkOVRxKBlgiUUn1b8CaC5hJB+1VDkWEhpMVF+DEopZTyv+BNBM1tBG1XDR2qbCArORrpYGZSpZTqC4I3ETT3GmqvRNBAVpK2Dyil+r7gTQQn6DV0qKJB2weUUkEheBNBB+MI6h0uyuoc2mNIKRUUgjcRdFAiKKjUrqNKqeARvInA0X4iyKuwiUDbCJRSwSB4E4GzDsKiIeT4U3CoQscQKKWCR/Amgg7WIsivaCA8VOgXH+nnoJRSyv+CNxF0sBbBocoGBiRFExKiYwiUUn1f8CaCDtYiyK+o1/YBpVTQCN5E4Kw/wRgCTQRKqeAQvInAUd/mGIIml5vimiaykrShWCkVHII3ETjr2hlD0AjoGAKlVPAI3kTQTq+hI11HszQRKKWCRPAmgnZ6DR1ZmUxLBEqpYOHTRCAi80Rkp4jsEZGftrPPHBHZICJbReQzX8ZzjHZ6DR2qbCA0ROifEOW3UJRSKpDCfHVgEQkF/gBcAOQDq0XkHWPMthb7JAF/BOYZYw6KSD9fxXOcdnoN5Vc00D8hirDQ4C0sKaWCiy+vdtOAPcaYfcYYB7AEuLzVPt8F3jTGHAQwxhT7MJ6j3C5wO9rsNXSookHbB5RSQcWXiSALyGvxPN+7raVRQLKIfCoia0XkhrYOJCK3iMgaEVlTUlJy8pF1sF5xfkW9tg8opYKKLxNBW/MzmFbPw4DTgW8BFwEPisio495kzGJjzFRjzNT09PSTj6yd9Yqdbg+F1Y0M1FHFSqkg4rM2AmwJYFCL5wOBgjb2KTXG1AF1IrISmATs8mFc7a5XXFjViMdo11GlVHDxZYlgNTBSRIaKSASwEHin1T5vA7NEJExEYoDpwHYfxmS1s15xSW0TAP3itceQUip4+KxEYIxxicgdwDIgFHjOGLNVRG71vv60MWa7iHwAbAI8wJ+MMVt8FVOzdlYnq6hzAJASG+HzEJRSqrfwZdUQxpj3gPdabXu61fPHgMd8GcdxHG03FpdrIlCqy5xOJ/n5+TQ2NgY6FAVERUUxcOBAwsPDO/0enyaCXstlq4AIO3bhmSOJIFkTgVKdlp+fT3x8PNnZ2YjoGh6BZIyhrKyM/Px8hg4d2un3BeeoKXc7iaDeQURYCLERoQEISqlTU2NjI6mpqZoEegERITU1tculs+BMBC57509oq0RQ6yAlJkL/oJXqIv0/03t057sIzkTQTomgot6h7QNKqaATnImggzYCTQRKqWATnInAfaRq6NiLfnmdQxuKlVLtcrlcgQ7BJ7TXUAvldQ5SNREo1W2/eHcr2wqqe/SYYwck8PCl406437e//W3y8vJobGzkxz/+MbfccgsffPAB999/P263m7S0ND7++GNqa2u58847WbNmDSLCww8/zJVXXklcXBy1tbUAvP766yxdupQXXniB733ve6SkpLB+/XqmTJnCNddcw1133UVDQwPR0dE8//zzjB49GrfbzX333ceyZcsQEW6++WbGjh3Lk08+yVtvvQXARx99xFNPPcWbb77Zo+foZAVnInAf31jsdHuobnSRHKOJQKlT0XPPPUdKSgoNDQ2cccYZXH755dx8882sXLmSoUOHUl5eDsAvf/lLEhMT2bx5MwAVFRUnPPauXbtYvnw5oaGhVFdXs3LlSsLCwli+fDn3338/b7zxBosXL2b//v2sX7+esLAwysvLSU5O5vbbb6ekpIT09HSef/55Fi1a5NPz0B3BmQhcTRASBiFHa8Yq6r2DyeI0ESjVXZ25c/eVJ554ovnOOy8vj8WLF3POOec096dPSUkBYPny5SxZsqT5fcnJySc89lVXXUVoqO1WXlVVxY033sju3bsREZxOZ/Nxb731VsLCwo75vOuvv56//OUvLFq0iFWrVvHiiy/20G/cc4IzEbgdx3cdPTKqWEsESp1yPv30U5YvX86qVauIiYlhzpw5TJo0iZ07dx63rzGmzS6WLbe17ocfG3t0gsoHH3yQc889l7feeovc3FzmzJnT4XEXLVrEpZdeSlRUFFdddVVzouhNgrOx2NUEYcc3FAMkx3Z+WLZSqneoqqoiOTmZmJgYduzYwVdffUVTUxOfffYZ+/fvB2iuGrrwwgt58sknm997pGooIyOD7du34/F4mksW7X1WVpZdWuWFF15o3n7hhRfy9NNPNzcoH/m8AQMGMGDAAH71q1/xve99r8d+554UnInA3XRciaCizhbvUmMj23qHUqoXmzdvHi6Xi4kTJ/Lggw8yY8YM0tPTWbx4Md/5zneYNGkS11xzDQA///nPqaioYPz48UyaNIkVK1YA8OijjzJ//nzOO+88MjMz2/2se++9l5/97GecddZZuN3u5u033XQTgwcPZuLEiUyaNIlXXnml+bXrrruOQYMGMXbsWB+dgZMjxrReK6Z3mzp1qlmzZs3JHeTNH8DBL+Guzc2bXlqVy4Nvb+WbB+bqNNRKdcH27dvJyckJdBi92h133MHkyZP5/ve/75fPa+s7EZG1xpipbe3f+yqr/MHV2EYbgS0RaK8hpVRPOv3004mNjeV3v/tdoENpV3AmArejjTEETSREhREeGpy1ZUop31i7dm2gQzih4LzquZramHnUqdNLKKWCUnAmgja6j1bWO0jSaiGlVBAKzkTQRvfRqgYnSTHadVQpFXyCMxG00X20st5JYrQmAqVU8AnOROBytF0i0ESglApCwZkIWpUIPB5DdaOWCJQKBnFxcYEOodcJzu6jrmO7j9Y0ujAGEjQRKHVy3v8pFG4+8X5d0X8CXPxozx6zF3C5XL1m3qEgLhEcrRqqarCDybTXkFKnnvvuu48//vGPzc8feeQRfvGLXzB37lymTJnChAkTePvttzt1rNra2nbf9+KLLzZPH3H99dcDUFRUxBVXXMGkSZOYNGkSX375Jbm5uYwfP775ff/93//NI488AsCcOXO4//77mT17No8//jjvvvsu06dPZ/LkyZx//vkUFRU1x7Fo0SImTJjAxIkTeeONN3j22We5++67m4/7zDPPcM8993T7vB3DGHNK/Tv99NPNSfv1IGPeu7f56ca8CjPkvqXmw62FJ39spYLMtm3bAvr569atM+ecc07z85ycHHPgwAFTVVVljDGmpKTEDB8+3Hg8HmOMMbGxse0ey+l0tvm+LVu2mFGjRpmSkhJjjDFlZWXGGGOuvvpq8z//8z/GGGNcLpeprKw0+/fvN+PGjWs+5mOPPWYefvhhY4wxs2fPNrfddlvza+Xl5c1xPfPMM+aee+4xxhhz7733mh//+MfH7FdbW2uGDRtmHA6HMcaYmTNnmk2bNrX5e7T1nQBrTDvX1d5RLvG3dksEWjWk1Klm8uTJFBcXU1BQQElJCcnJyWRmZnL33XezcuVKQkJCOHToEEVFRfTv37/DYxljuP/++4973yeffMKCBQtIS0sDjq418MknnzSvLxAaGkpiYuIJF7o5MvkdQH5+Ptdccw2HDx/G4XA0r53Q3poJ5513HkuXLiUnJwen08mECRO6eLbaFnyJwJjjRhZX1ttEoI3FSp2aFixYwOuvv05hYSELFy7k5ZdfpqSkhLVr1xIeHk52dvZxawy0pb33mXbWGmhLWFgYHo+n+XlHaxvceeed3HPPPVx22WV8+umnzVVI7X3eTTfdxK9//WvGjBnToyudBV8bgdsJmGMSQXOJQBOBUqekhQsXsmTJEl5//XUWLFhAVVUV/fr1Izw8nBUrVnDgwIFOHae9982dO5fXXnuNsrIy4OhaA3PnzuWpp54CwO12U11dTUZGBsXFxZSVldHU1MTSpUs7/Lwjaxv8+c9/bt7e3poJ06dPJy8vj1deeYVrr722s6fnhIIwEXgXrg89PhForyGlTk3jxo2jpqaGrKwsMjMzue6661izZg1Tp07l5ZdfZsyYMZ06TnvvGzduHA888ACzZ89m0qRJzY20jz/+OCtWrGDChAmcfvrpbN26lfDwcB566CGmT5/O/PnzO/zsRx55hKuuuopZs2Y1VztB+2smAFx99dWcddZZnVpis7OCbz2CujJ4bBhc/FuY/gMAfv3edl5clcuOX17cQ1EqFTx0PQL/mj9/PnfffTdz585td5+urkcQxCWCo43FlfUObR9QSvVqlZWVjBo1iujo6A6TQHcEX2Oxy5sIWrURaCJQKnhs3ry5eSzAEZGRkXz99dcBiujEkpKS2LVrl0+OHXyJwG0XqW/dfTQpWgeTKdVdXelV0xtMmDCBDRs2BDoMn+hOdX/wVQ21USKorHdqQ7FS3RQVFUVZWVm3LkCqZxljKCsrIyqqa+uuB3GJ4GgiqG5wkpSliUCp7hg4cCD5+fmUlJQEOhSFTcwDBw7s0nuCLxE0lwhaNBZrG4FS3RYeHt48IladmnxaNSQi80Rkp4jsEZGftvH6HBGpEpEN3n8P+TIe4LhxBA6Xh3qHWweTKaWCls9KBCISCvwBuADIB1aLyDvGmG2tdv2nMWa+r+I4jstbNeQtERwZTJao8wwppYKUL0sE04A9xph9xhgHsAS43Ief1zmtSgTNiUBLBEqpIOXLNoIsIK/F83xgehv7zRSRjUAB8O/GmK2tdxCRW4BbvE9rRWRnN2NKA0oB+MWxs/Z9+zfdPGLPORpb76JxdU1vjQt6b2waV9d0N64h7b3gy0TQVqfi1v3L1gFDjDG1InIJ8Hdg5HFvMmYxsPikAxJZ094Q60DrrbFpXF3TW+OC3hubxtU1vojLl1VD+cCgFs8HYu/6mxljqo0xtd7H7wHhIpKGUkopv/FlIlgNjBSRoSISASwE3mm5g4j0F+9wRBGZ5o2nzIcxKaWUasVnVUPGGJeI3AEsA0KB54wxW0XkVu/rTwMLgNtExAU0AAuNb4cnnnT1kg/11tg0rq7prXFB741N4+qaHo/rlJuGWimlVM8KvrmGlFJKHUMTgVJKBbmgSQQnmu7Cj3EMEpEVIrJdRLaKyI+92x8RkUMtptu4JACx5YrIZu/nr/FuSxGRj0Rkt/dnz62P1/m4Rrc4LxtEpFpE7grEOROR50SkWES2tNjW7jkSkZ95/+Z2ishFfo7rMRHZISKbROQtEUnybs8WkYYW5+1pP8fV7vfmr/PVQWx/bRFXrohs8G73yznr4Prg278xY0yf/4dtrN4LDAMigI3A2ADFkglM8T6OB3YBY4FHsAPqAnmecoG0Vtt+C/zU+/inwG96wXdZiB0c4/dzBpwDTAG2nOgceb/XjUAkMNT7Nxjqx7guBMK8j3/TIq7slvsF4Hy1+b3583y1F1ur138HPOTPc9bB9cGnf2PBUiLoNdNdGGMOG2PWeR/XANuxo7B7q8uBP3sf/xn4duBCAWAusNcYcyAQH26MWQmUt9rc3jm6HFhijGkyxuwH9mD/Fv0SlzHmQ2OMy/v0K+xYHr9q53y1x2/n60Sxebu1Xw286qvPbyem9q4PPv0bC5ZE0NZ0FwG/+IpINjAZOLI+3h3eYvxzgaiCwY78/lBE1nqn9QDIMMYcBvtHCvQLQFwtLeTY/5yBPmfQ/jnqTX93/wq83+L5UBFZLyKficisAMTT1vfWm87XLKDIGLO7xTa/nrNW1wef/o0FSyLozHQXfiUiccAbwF3GmGrgKWA4cBpwGFss9bezjDFTgIuB20XknADE0C6xAxMvA/7m3dQbzllHesXfnYg8ALiAl72bDgODjTGTgXuAV0QkwY8htfe99Yrz5XUtx95w+PWctXF9aHfXNrZ1+ZwFSyI44XQX/iQi4dgv+WVjzJsAxpgiY4zbGOMBnsGHReL2GGMKvD+Lgbe8MRSJSKY37kyg2N9xtXAxsM4YUwS945x5tXeOAv53JyI3AvOB64y3UtlbjVDmfbwWW688yl8xdfC9Bfx8AYhIGPAd4K9HtvnznLV1fcDHf2PBkghOON2Fv3jrHp8Fthtj/l+L7ZktdrsC2NL6vT6OK1ZE4o88xjY0bsGepxu9u90IvO3PuFo55i4t0OeshfbO0TvAQhGJFJGh2AkVv/FXUCIyD7gPuMwYU99ie7rY9UIQkWHeuPb5Ma72vreAnq8Wzgd2GGPyj2zw1zlr7/qAr//GfN0K3lv+AZdgW+D3Ag8EMI6zsUW3TcAG779LgJeAzd7t7wCZfo5rGLb3wUZg65FzBKQCHwO7vT9TAnTeYrDzUCW22Ob3c4ZNRIcBJ/Zu7PsdnSPgAe/f3E7gYj/HtQdbf3zk7+xp775Xer/jjdgZgC/1c1ztfm/+Ol/txebd/gJwa6t9/XLOOrg++PRvTKeYUEqpIBcsVUNKKaXaoYlAKaWCnCYCpZQKcpoIlFIqyGkiUEqpIKeJQKlWRMQtx8522mOz1XpnsQzUeAel2uSzpSqVOoU1GGNOC3QQSvmLlgiU6iTv/PS/EZFvvP9GeLcPEZGPvZOofSwig73bM8SuA7DR++9M76FCReQZ73zzH4pIdMB+KaXQRKBUW6JbVQ1d0+K1amPMNOBJ4H+9254EXjTGTMRO7PaEd/sTwGfGmEnYee+3erePBP5gjBkHVGJHrSoVMDqyWKlWRKTWGBPXxvZc4DxjzD7vxGCFxphUESnFTpPg9G4/bIxJE5ESYKAxpqnFMbKBj4wxI73P7wPCjTG/8sOvplSbtESgVNeYdh63t09bmlo8dqNtdSrANBEo1TXXtPi5yvv4S+yMtgDXAZ97H38M3AYgIqF+nvNfqU7TOxGljhct3kXLvT4wxhzpQhopIl9jb6Ku9W77EfCciPwEKAEWebf/GFgsIt/H3vnfhp3tUqleRdsIlOokbxvBVGNMaaBjUaonadWQUkoFOS0RKKVUkNMSgVJKBTlNBEopFeQ0ESilVJDTRKCUUkFOE4FSSgW5/w+emfosaWaEagAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkD4lB0Gc5kq"
      },
      "source": [
        "\n",
        "\n",
        "# model.save(\"efficient_final_model\")\n",
        "# model = tf.keras.models.load_model(\"mobileNet_model\")\n",
        "\n",
        "\n",
        "# image_inputs = ct.ImageType(shape=(1, 224, 224, 3))\n",
        "# coreml_model_file = 'mobilenet.mlmodel'\n",
        "# output = ['InceptionV1/Logits/Predictions/Softmax']\n",
        "\n",
        "\n",
        "# coreml_model = ct.convert(model, \n",
        "#                           inputs=[image_inputs], \n",
        "#                           outputs=output)\n",
        "\n",
        "# coreml_model.save(coreml_model_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR8NwKsHgrxi"
      },
      "source": [
        "!pip3 install coremltools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbpxqCVFg2Y1"
      },
      "source": [
        "import coremltools as ct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVsXJZCAc6m7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d6638da-52ca-423a-e8c0-19fae5a18580"
      },
      "source": [
        "\n",
        "model.save(\"efficientNet_model\")\n",
        "tf_model_path = model\n",
        "image_inputs = ct.ImageType(shape=(1, 224, 224, 3))\n",
        "coreml_model_file = 'efficientNet.mlmodel'\n",
        "classifier_config = ct.ClassifierConfig(categories)\n",
        "\n",
        "\n",
        "coreml_model = ct.convert(tf_model_path, \n",
        "                          inputs=[image_inputs], source = \"tensorflow\", classifier_config=classifier_config\n",
        "                          )\n",
        "\n",
        "coreml_model.save(coreml_model_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running TensorFlow Graph Passes: 100%|██████████████████████████████████████████████| 5/5 [00:05<00:00,  1.14s/ passes]\n",
            "Converting Frontend ==> MIL Ops: 100%|██████████████████████████████████████████| 1318/1318 [00:07<00:00, 175.51 ops/s]\n",
            "Running MIL optimization passes: 100%|████████████████████████████████████████████| 18/18 [00:05<00:00,  3.17 passes/s]\n",
            "Translating MIL ==> MLModel Ops: 100%|██████████████████████████████████████████| 1931/1931 [00:13<00:00, 146.30 ops/s]\n"
          ]
        }
      ]
    }
  ]
}